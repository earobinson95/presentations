---
# PLEASE SEE THE README for in depth description github.com/brentthorne/posterdown
# https://github.com/brentthorne/posterdown/wiki/posterdown_betterland
poster_height: "29in"
poster_width: "48in"
font_family: 'Rasa'
#ESSENTIALS
title: 'Eye Fitting Straight Lines in the Modern Era'
author:
  - name: Emily A. Robinson
    affil:
    email: emily.robinson@huskers.unl.edu
    github: earobinson95
    # website: www.emilyarobinson.com
    orcid: 0000-0001-9800-7304
    main: true
affiliation:
  - num: 1
    address: Department of Statistics, University of Nebraska - Lincoln
#STYLE & FORMATTING
title_textsize: "70pt"
author_textsize: "1.17em"
authorextra_textsize: "32px"
affiliation_textsize: "25px"
affiliation_textcol: '#00000080'
caption_textsize: "20pt"
#Middle of the poster
main_fontfamily: "Special Elite"
main_textsize: "85pt"
main_width: 0.3
main_picwidth: 65%
main_findings:
  - "How do statistical regression results compare to intuitive, visually fitted results?"
  - '![](images\eyefitting_example.gif){.main_pic}'
  - '![](images\qr-codes.png){.main_pic}'
# logoleft_name: '![](images\youdrawit-mobile-qr-code.png){.main-img-left}'
# logoright_name: '![](images\youdrawit-mobile-qr-code.png){.main-img-right}'
# POSTER BODY OPTIONS
primary_colour: '#8b0000' # sets middle section color to gradient from (top)
secondary_colour: '#8b0000' # sets middle section color to gradient to (bottom)
accent_colour: "#5d19c4"
body_bgcol: "#ffffff"
body_textsize: "33px"
body_textcol: "#000000"
reference_textsize: "14px"
# STANDARD OPTIONS
output: 
  posterdown::posterdown_betterland:
    self_contained: false
    pandoc_args: --mathjax
    highlight: espresso
    # highlight: haddock
    number_sections: false
link-citations: true
bibliography: bibliography.bib
knit: pagedown::chrome_print
---

```{r, include=FALSE}
knitr::opts_chunk$set(results = 'asis',
                      echo = FALSE,
                      warning = FALSE,
                      tidy = FALSE,
                      message = FALSE,
                      fig.align = 'center',
                      out.width = "100%")
options(knitr.table.format = "html") 
library(tidyverse)
library(patchwork)
```

```{r, include=FALSE}
knitr::write_bib(c('mosteller1981eye', 'finney1951subjective','ciccione2021can'), 'bibliography.bib')
```

```{r}
library(magrittr)
set.seed(2)
corrCoef = 0.5 # sample from a multivariate normal, 10 datapoints
dat = MASS::mvrnorm(10,c(0,0),Sigma = matrix(c(1,corrCoef,2,corrCoef),2,2))
dat[,1] = dat[,1] - mean(dat[,1]) # it makes life easier for the princomp
dat[,2] = dat[,2] - mean(dat[,2])
dat = data.frame(x1 = dat[,1],x2 = dat[,2])
# Calculate the first principle component
# see http://stats.stackexchange.com/questions/13152/how-to-perform-orthogonal-regression-total-least-squares-via-pca
v = dat%>%prcomp%$%rotation
x1x2cor = bCor = v[2,1]/v[1,1]
x1tox2 = coef(lm(x1~x2,dat))
x2tox1 = coef(lm(x2~x1,dat))
slopeData = data.frame(slope = c(x1x2cor,x2tox1[2]),
                       type=c("PC", "OLS"))
# We want this to draw the neat orthogonal lines.
pointOnLine = function(inp){
  # y = a*x + c (c=0)
  # yOrth = -(1/a)*x + d
  # yOrth = b*x + d
  x0 = inp[1] 
  y0 = inp[2] 
  a = x1x2cor
  b = -(1/a)
  c = 0
  d = y0 - b*x0
  x = (d-c)/(a-b)
  y = -(1/a)*x+d
  return(c(x,y))
}
points = apply(dat,1,FUN=pointOnLine)
segmeData = rbind(data.frame(x=dat[,1],y=dat[,2],xend=points[1,],yend=points[2,],type = "PC"),
                  data.frame(x=dat[,1],y=dat[,2],yend=dat[,1]*x2tox1[2],xend=dat[,1],type="OLS"))
pca_vs_ols_plot <- dat %>%
ggplot(aes(x1,x2))+
  geom_point()+
  geom_abline(data=slopeData,aes(slope = slope,intercept=0,color=type, linetype=type), size = 1.2)+
  geom_segment(data=segmeData,aes(x=x,y=y,xend=xend,yend=yend,color=type, linetype=type))+
  facet_grid(.~type)+
  coord_equal()+
  scale_x_continuous("x") +
  scale_y_continuous("y") +
  theme_bw(base_size = 14) +
  theme(aspect.ratio = 1,
        legend.position = "none",
        axis.text    = element_text(size = 12),
        axis.title   = element_text(size = 12),
        legend.title = element_blank(),
        # legend.text  = element_text(size = 10),
        # strip.text = element_text(size = 8, margin = margin(0.1,0,0.1,0, "cm")),
        # strip.background = element_rect(size = 0.8),
        legend.key.size = unit(1, "line")
        ) +
  scale_color_manual(values = c("steelblue", "orange"), labels = c("OLS", "PCA")) +
  scale_linetype_manual(values = c("solid", "dashed"), labels = c("OLS", "PCA"))
          
eyefitting_example_sim <- read.csv("data/youdrawit/youdrawit-eyefitting-simdata-example.csv")
eyefitting_example_simplot <- eyefitting_example_sim %>%
  filter(data == "point_data") %>%
  filter(dataset %in% c("F", "N", "S") | (x < 16 & x > 4)) %>%
  mutate(dataset = factor(dataset, levels = c("S", "F", "V", "N"))) %>%
  dplyr::rename(`Parameter Choice` = dataset) %>%
  ggplot(aes(x = x, y = y)) +
  geom_point(size = 1) +
  facet_wrap(~`Parameter Choice`, ncol = 4) +
  theme_bw(base_size = 14) +
  theme(aspect.ratio = 1,
  legend.position = "none",
  plot.title   = element_text(size = 12, hjust = 0),
  axis.text    = element_text(size = 12),
  axis.title   = element_text(size = 12),
  legend.title = element_text(size = 12),
  legend.text  = element_text(size = 12),
  # strip.text = element_text(size = 5, margin = margin(0.05,0,0.05,0, "cm")),
  # strip.background = element_rect(size = 0.5),
  legend.key.size = unit(1, "line")
) +
  scale_y_continuous(breaks = seq(-10, 20, 5))
eyefitting_model_data <- read.csv("data/youdrawit/youdrawit-eyefitting-model-data.csv") %>%
  dplyr::rename(`Parameter Choice` = parm_id)
trial.feedback <- read.csv("data/youdrawit/youdrawit-eyefitting-example-feedback.csv") %>%
    mutate(`Parameter Choice` = "F")
trial.sim <- read.csv("data/youdrawit/youdrawit-eyefitting-example-simulated.csv") %>%
    mutate(`Parameter Choice` = "F")
    
trial_plot <- trial.feedback %>%
  ggplot(aes(x = x)) +
  geom_point(data = trial.sim, aes(y = y), alpha = 0.7) +
  geom_line(aes(y = y, color = "OLS", linetype = "OLS")) +
  geom_line(aes(y = ypca, color = "PCA", linetype = "PCA")) +
  geom_line(aes(y = ydrawn, color = "Drawn", linetype = "Drawn")) +
  facet_wrap(~`Parameter Choice`) +
  theme_bw(base_size = 14) +
  theme(aspect.ratio = 1,
        legend.position = "right",
        axis.text    = element_text(size = 12),
        axis.title   = element_text(size = 12),
        legend.title = element_text(size = 12),
        legend.text  = element_text(size = 12),
        # strip.text = element_text(size = 8, margin = margin(0.1,0,0.1,0, "cm")),
        # strip.background = element_rect(size = 0.8),
        legend.key.size = unit(1, "line")
        ) +
  scale_x_continuous(limits = c(0,20)) +
  scale_y_continuous(limits = c(-5, 17), breaks = seq(-5, 15, 5)) +
  scale_color_manual("", values = c("black", "steelblue", "orange")) +
  scale_linetype_manual("", values = c("dashed", "solid", "solid"))
eyefitting.preds.lmer <- read.csv("data/youdrawit/youdrawit-eyefitting-lmerpred-data.csv")
# Plot Predictions
eyefitting.lmer.plot <- eyefitting.preds.lmer %>%
  filter((parm_id %in% c("F", "N", "S") | (x <= 16 & x >= 4))) %>%
  mutate(parm_id = factor(parm_id, levels = c("S", "F", "V", "N"))) %>%
  dplyr::rename(`Parameter Choice` = parm_id) %>%
  ggplot(aes(x = x)) +
  geom_line(data = eyefitting_model_data, aes(x = x, y = residualols, group = plotID, color = "OLS"), alpha = 0.1) +
  geom_line(data = eyefitting_model_data, aes(x = x, y = residualpca, group = plotID, color = "PCA"), alpha = 0.1) +
  geom_ribbon(aes(ymin = asymp.LCL.ols, ymax = asymp.UCL.ols, fill = "OLS"), color = NA, alpha = 0.7) +
  geom_line(aes(y = emmean.ols, color = "OLS")) +
  geom_ribbon(aes(ymin = asymp.LCL.pca, ymax = asymp.UCL.pca, fill = "PCA"), color = NA, alpha = 0.7) +
  geom_line(aes(y = emmean.pca, color = "PCA")) +
  geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.5) +
  facet_wrap(~`Parameter Choice`, ncol = 4) +
  theme_bw(base_size = 14) +
  theme(aspect.ratio = 1,
        legend.position = "right",
        plot.title   = element_text(size = 12, hjust = 0),
        axis.text    = element_text(size = 12),
        axis.title   = element_text(size = 12),
        legend.title = element_text(size = 12),
        legend.text  = element_text(size = 12),
        # strip.text = element_text(size = 5, margin = margin(0.05,0,0.05,0, "cm")),
        # strip.background = element_rect(size = 0.5),
        legend.key.size = unit(1, "line")
        ) +
  scale_y_continuous("Residual") +
  scale_color_manual("Individual participant \nresiduals", values = c("steelblue", "orange"), labels = c("OLS", "PCA")) +
  scale_fill_manual("LMER fitted trend", values = c("steelblue", "orange"), labels = c("OLS", "PCA")) 
eyefitting.grid.gamm <- read.csv("data/youdrawit/youdrawit-eyefitting-gammpred-data.csv")
eyefitting.gamm.plot <- eyefitting.grid.gamm %>%
  filter((parm_id %in% c("F", "N", "S") | (x <= 16 & x >= 4))) %>%
  mutate(parm_id = factor(parm_id, levels = c("S", "F", "V", "N"))) %>%
  dplyr::rename(`Parameter Choice` = parm_id) %>%
  ggplot(aes(x = x)) +
  geom_line(data = eyefitting_model_data, aes(x = x, y = residualols, group = plotID, color = "OLS"), alpha = 0.1) +
  geom_line(data = eyefitting_model_data, aes(x = x, y = residualpca, group = plotID, color = "PCA"), alpha = 0.1) +
  geom_ribbon(aes(ymin = ols.lower, ymax = ols.upper, fill = "OLS"), color = NA, alpha = 0.5) +
  geom_line(aes(y = ols.pred, color = "OLS")) +
  geom_ribbon(aes(ymin = pca.lower, ymax = pca.upper, fill = "PCA"), color = NA, alpha = 0.5) +
  geom_line(aes(y = pca.pred, color = "PCA")) +
  geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.5) +
  facet_wrap(~`Parameter Choice`, ncol = 4) +
  theme_bw(base_size = 14) +
  theme(aspect.ratio = 1,
        legend.position = "right",
        plot.title   = element_text(size = 12, hjust = 0),
        axis.text    = element_text(size = 12),
        axis.title   = element_text(size = 12),
        legend.title = element_text(size = 12),
        legend.text  = element_text(size = 12),
        # strip.text = element_text(size = 5, margin = margin(0.05,0,0.05,0, "cm")),
        # strip.background = element_rect(size = 0.5),
        legend.key.size = unit(1, "line")
        ) +
  scale_y_continuous("Residual") +
  scale_color_manual("Individual participant \nresiduals", values = c("steelblue", "orange"), labels = c("OLS", "PCA")) +
  scale_fill_manual("GAMM fitted trend", values = c("steelblue", "orange"), labels = c("OLS", "PCA")) 
```

## Introduction

We all use statistical graphics, but how do we know that the graphics we use are communicating properly?
We need human testing of graphics in order to draw broad conclusions, develop guidelines for graphical design, and improve graphical communication.

### Fitting Trends by Eye

Fitting lines by eye through a set of points has been explored since the 20th century [@finney1951subjective; @mosteller1981eye; @ciccione2021can]. 
Common methods of fitting trends by eye involved maneuvering a string, black thread, or ruler until the fit is suitable, then drawing the line through the set of points. In 2015, the New York Times introduced an interactive feature, called You Draw It. Readers were asked to input their own assumptions about various metrics and compare how these assumptions relate to reality.

### Objectives

1. Implement and validate the 'You Draw It' feature as a way to measure the patterns we see in data.
2. Introduce a method for statistically modeling the participant drawn lines.

## Methods

In the study, participants were shown an interactive scatter-plot along with the prompt, *“Use your mouse to fill in the trend in the yellow box region.”* Data Driven Documents (D3), a JavaScript-based graphing framework that facilitates user interaction, was used to create the 'You Draw It' visual. 
In order to allow for user interaction and data collection, we integrated the D3 visual into Shiny using the @r2d3 package. 

The experiment was conducted through an R Shiny app found [`here`](https://emily-robinson.shinyapps.io/you-draw-it-validation-applet/).

All data processing was conducted in R statistical software. 
A total of $N = 30$ points $(x_i, y_i), i = 1,...N$ were generated for $x_i \in [x_{min}, x_{max}]$ where $x$ and $y$ have a linear relationship.
Data were simulated based on a linear model with additive errors: 
\begin{align}
y_i & = \beta_0 + \beta_1 x_i + e_i \\
\text{with } e_i & \sim N(0, \sigma^2). \nonumber
\end{align} 

Model equation parameters, $\beta_0$ and $\beta_1$, were selected to reflect the four data sets (F, N, S, and V) used in @mosteller1981eye.

```{r eyefitting-simplot, fig.height = 3, fig.width = 9, fig.cap = "Example of simulated data points displayed in a scatter-plot illustrating the trends associated with the four selected parameter choices.", out.width="80%", include = T}
eyefitting_example_simplot
```

<br><br><br><br>

## Results

A total of 35 individuals completed 119 unique 'You Draw It' task plots. For each participant and parameter choice, the final feedback data set contained $x, y_{drawn}, \hat y_{OLS}$, and $\hat y_{PCA}$.

### Fitted Regression Lines

We compared the participant drawn line to two regression lines determined by ordinary least squares (OLS) regression and regression based on the principal axis (PCA).

```{r ols-vs-pca-example, fig.height = 3, fig.width = 9, fig.cap="Left: Comparison between an OLS regression line which minimizes the vertical distance of points from the line and a regression line based on the principal axis which minimizes the Euclidean distance of points (orthogonal) from the line. Right: Illustrates the data associated with and collected for one `You Draw It' task plot. Trend-lines include the participant drawn line (dashed black), the OLS regression line (solid steelblue) and the PCA regression line based on the principal axis (solid orange).", message=FALSE, warning=FALSE, out.width="100%"}
pca_vs_ols_plot + trial_plot + 
  plot_layout(widths = c(2, 1))
```

```{r eyefitting-example-plot, fig.height = 4, fig.width = 4, fig.cap = "Illustrates the data associated with and collected for one `You Draw It' task plot. Trend-lines include the participant drawn line (dashed black), the OLS regression line (solid steelblue) and the PCA regression line based on the principal axis (solid orange).", out.width="50%", include = F}
trial_plot
```

### Residual Trends

Using a generalized additive mixed model (GAMM), comparisons of vertical residuals in relation to the OLS fitted values ($e_{OLS} = y_{drawn} - \hat y_{OLS}$) and PCA fitted values ($e_{PCA} = y_{drawn} - \hat y_{PCA}$) were made across the domain.

```{r, eyefitting-results, fig.height = 3.5, fig.width = 9, out.width = "100%", fig.cap = "Estimated trend line of the residuals between the participant drawn points and fitted values for both the OLS (blue) regression line and PCA (orange) regression line. Estimated residual trends with 95\\% confidence bands are overlaid on the observed individual participant residuals."}
# eyefitting.lmer.plot/
eyefitting.gamm.plot
```

Estimated drawn trend-lines followed closer to the regression line based on the principal axes than the OLS regression line and is most prominent in data simulated with large variances.
The reproducibility of these results serve as validation of the 'You Draw It' tool and method.

## Future Work

Further investigation is necessary to implement this method in non-linear settings and with real data in order to facilitate scientific communication.
This tool could also be used to evaluate human ability to extrapolate data from trends.
In the future, we intend to create an R package designed for easy implementation of 'You Draw It' task plots in order to make this tool accessible to other researchers.
 
## References